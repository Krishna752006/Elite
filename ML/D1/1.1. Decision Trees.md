# Decision Trees — Notes

### Reference

YouTube: [https://www.youtube.com/watch?v=_L39rN6gz7Y](https://www.youtube.com/watch?v=_L39rN6gz7Y)

## 1. What is a Decision Tree

A **decision tree** is a supervised learning algorithm that predicts outcomes by **recursively splitting data** based on feature values so that the resulting subsets are increasingly **pure** with respect to the target.

* Two types:

  * **Classification Tree** → categorical output
  * **Regression Tree** → continuous output
* Structure:

  * Root node → internal decision nodes → leaf nodes
* Each internal node represents:

  * A **feature**
  * A **threshold**
* Leaf nodes store:

  * Class label (classification)
  * Mean target value (regression)

## 2. Core Objective

At each node:

> Choose the **feature + threshold** that produces the **maximum reduction in impurity (or error)**.

This is done using a **greedy algorithm** (locally optimal decisions).

## 3. Impurity (Classification Trees)

Impurity measures how **mixed** the classes are in a node.

### 3.1 Gini Impurity

$$
\text{Gini} = 1 - \sum_{i=1}^{C} p_i^2
$$


* ( p_i ) = probability of class ( i )
* Gini = 0 → node is perfectly pure
* Faster to compute than entropy

Example:

* [yes, yes, no, no] → Gini = 0.5
* [yes, no, no, no] → Gini = 0.375

#### Important correction

❌ **“Gini counters imbalanced datasets”** — **Not exactly**

* Gini does **not solve class imbalance**
* It is only *less sensitive* than entropy
* Trees still bias toward majority class

### 3.2 Entropy

$$
\text{Entropy} = - \sum_{i=1}^{C} p_i \log_2(p_i)
$$

* Measures uncertainty (information theory)
* Entropy = 0 → pure node
* Higher entropy → more mixed data

#### Important correction

❌ **“If too much entropy you can’t take a decision”** — **Incorrect**

* High entropy means the node is mixed
* Mixed nodes are exactly where trees **should split**
* The goal is to **reduce entropy**, not avoid it

### 3.3 Information Gain

$$
\text{Information Gain}
= \text{Entropy(parent)} - \text{Entropy(after split)}
$$

Maximize information gain ⇔ minimize weighted entropy.

## 4. Weighted Impurity (Why It’s Necessary)

After a split, child nodes usually have **different sizes**.

Weighted impurity measures **expected impurity after the split**:

$$
\text{Impurity}_{\text{split}}
= \frac{n_L}{n} \, I(L) + \frac{n_R}{n} \, I(R)
$$

Without weighting:

* Small pure nodes could dominate decisions unfairly.

## 5. Splitting Logic (Conceptual Algorithm)

For each node:

1. Iterate over all features
2. For each feature, try all possible thresholds
3. Split data into left (≤ threshold) and right (> threshold)
4. Compute weighted impurity
5. Choose split with **minimum impurity**
6. Recurse on children

This continues until stopping criteria are met.

## 6. Stopping Criteria (Critical)

Decision trees **overfit by default**.

Common stopping rules:

* Maximum depth reached
* Node impurity = 0
* No impurity improvement

Pruning or depth control is **mandatory** in practice.

## 7. Regression Trees vs Linear Regression

The statement *“for regression, linear regression is preferred”* is **not generally correct**.

### Linear Regression is better when:

* Relationship is approximately linear
* Feature effects are additive
* Model assumptions hold

### Regression Trees are better when:

* Strong non-linear relationships exist
* Threshold-based logic makes sense
* Minimal assumptions are desired

## 8. Key Blind Spots to Be Aware Of

1. **Greedy ≠ optimal**
   Trees optimize locally, not globally.

2. **High variance**
   Small data changes can drastically change the tree.

3. **Overfitting risk**
   Deep trees memorize data unless constrained.

These issues motivate **ensemble methods**.

## 9. Why Trees Still Matter Today

❌ **“NNs are faster and therefore decision trees are not used much”** — **Misleading**

Reality:

* Decision trees are **widely used**
* Especially dominant in **tabular / structured data**

Modern ML on tabular data is dominated by:

* **Random Forest**
* **Gradient Boosted Trees**

  * XGBoost
  * LightGBM
  * CatBoost

These are:

> **Decision trees + ensemble learning**,
> not neural networks.

Neural networks dominate:

* Vision
* Speech
* Language

Trees dominate:

* Business data
* Finance
* Recommendations
* Structured prediction problems

## 10. Explainability

Decision trees are a classic example of **Explainable AI**:

* Clear if–else rules
* Explicit feature thresholds
* Transparent decision paths

This is a major advantage over neural networks.

## 11. Routing Analogy (Decision Trees vs MoE Routers)

⚠️ **“DeepSeek router is similar to decision trees”**

* Conceptually similar: **conditional routing**
Analogy is valid, equivalence is not.

## 12. Final Takeaways

* Decision trees split data to reduce impurity or error.
* Gini and entropy quantify class mixing.
* Weighted impurity ensures fair evaluation of splits.
* High entropy is not a problem; it drives learning.
* Single trees are interpretable but unstable.
* Tree ensembles are state-of-the-art for tabular ML.
* Regression ≠ linear regression by default.
* Decision trees remain fundamental to modern ML.